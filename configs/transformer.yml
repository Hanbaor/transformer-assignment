#数据设置
src_language: 'de'       # 源语言: 德语
tgt_language: 'en'       # 目标语言: 英语
vocab_min_freq: 1        # 单词进入词典的最低频率，过滤掉稀有词

#模型超参数
d_model: 256             # 嵌入维度/模型隐藏层维度，词向量的长度
n_heads: 8               # 多头注意力的头数，必须能被 d_model 整除
d_ff: 1024               # 前馈网络的中间层维度
n_encoder_layers: 3      # 编码器的层数
n_decoder_layers: 3      # 解码器的层数
dropout: 0.1            # Dropout 比率，防止过拟合

#训练设置
batch_size: 32          # 每批训练的样本数
epochs: 10               # 总共训练的轮数
lr: 0.0001               # 学习率
optimizer: 'AdamW'       # 优化器，AdamW 是 Adam 的改进版，效果更好
grad_clip: 1.0           # 梯度裁剪阈值，防止梯度爆炸
seed: 42                 # 随机种子，保证实验可复现
device: 'cuda'           # 使用 'cuda' (GPU) 或 'cpu'

#输出设置
output_dir: 'results'                   # 保存结果的文件夹
model_save_name: 'transformer.pt'       # 保存最佳模型的名字
plot_save_name: 'training_curves.png'   # 保存训练曲线图的名字

tokenizer: spacy_blank
#debug_small_train: 100000
max_seq_len: 60
num_workers: 0
compile: false
compile_mode: reduce-overhead

vocab_cache_dir: 'results/vocabs'